{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =*=coding: utf-8 -*-\n",
    "# libraries\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# config\n",
    "np.random.seed(42)  # get random seed\n",
    "\n",
    "\n",
    "# get random seed based on time\n",
    "def randomize():\n",
    "    np.random.seed(time.time())\n",
    "\n",
    "\n",
    "# set hyperparmeters\n",
    "RND_MEAN = 0  # for default weight, bias\n",
    "RND_STD = .003  # for default weight, bias\n",
    "MB_SIZE = 10  # mini batch size\n",
    "LEARNING_RATE = .001  # learning rate\n",
    "data_path = '../data/abalone.csv'\n",
    "\n",
    "\n",
    "\n",
    "def load_data_set(path, input_cnt, output_cnt):\n",
    "    \"\"\"load data set\n",
    "    load data set from path\n",
    "    make one hot vector with gender column\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    input_cnt: int\n",
    "        number of x columns\n",
    "    output_cnt: int\n",
    "        number of target columns\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    data: numpy ndarray\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        next(csv_reader, None)  # skip header\n",
    "        row_list = []\n",
    "        for n, row in enumerate(csv_reader):\n",
    "            row_list.append(row)\n",
    "    # initiate data array\n",
    "    data = np.zeros(shape=[len(row_list), input_cnt+output_cnt])\n",
    "\n",
    "    for n, row in enumerate(row_list):\n",
    "        # make one hot vector for gender column\n",
    "        if row[0] == 'I':\n",
    "            data[n, 0] = 1\n",
    "        elif row[0] == 'M':\n",
    "            data[n, 1] = 1\n",
    "        elif row[0] == 'F':\n",
    "            data[n, 2] = 1\n",
    "        else:\n",
    "            print(\"invalid gender\")\n",
    "            \n",
    "        data[n, 3:] = row[1:]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def init_model(rnd_mean, rnd_std, input_cnt, output_cnt):\n",
    "    \"\"\"initiate model parameters\n",
    "        initiate weight, bias with hyperparameters\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "        rnd_mean: float\n",
    "        rnd_std: float\n",
    "        input_cnt: int\n",
    "        output_cnt: int\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "        weigh_arr: np.ndarray\n",
    "        bias_arr: np.adarray\n",
    "    \"\"\"\n",
    "    weight_arr = np.random.normal(loc=rnd_mean, scale=rnd_std, size=[input_cnt, output_cnt])\n",
    "    bias_arr = np.zeros(shape=[output_cnt])\n",
    "\n",
    "    return weight_arr, bias_arr\n",
    "\n",
    "\n",
    "def train_and_test(data, epoch_cnt, mb_size, report, weight_arr, bias_arr):\n",
    "    \"\"\"train and test\n",
    "    run nuralnet\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    epoch_cnt: int\n",
    "        number of epoches\n",
    "    mb_size: int\n",
    "        mini batch size\n",
    "    report: int\n",
    "        print test loss, accuracy each `report` count\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    \"\"\"\n",
    "    shuffle_map, step_cnt, test_begin_idx = arange_data(data=data, mb_size=mb_size)\n",
    "    test_x, test_y = get_test_data(data=data,\n",
    "                                      shuffle_map=shuffle_map,\n",
    "                                      test_begin_idx=test_begin_idx,\n",
    "                                      output_cnt=1)\n",
    "\n",
    "    for epoch in range(epoch_cnt):\n",
    "        loss_list, acc_list = [], []\n",
    "        # get mini batch for this step\n",
    "        for step in range(step_cnt):\n",
    "            train_x, train_y = get_train_data(data=data,\n",
    "                                                 mb_size=mb_size,\n",
    "                                                 shuffle_map=shuffle_map,\n",
    "                                                 test_begin_idx=test_begin_idx,\n",
    "                                                 output_cnt=1,\n",
    "                                                 step_idx=step)\n",
    "            # print(f'train_x: {train_y}')\n",
    "            # run train and get loss, acc\n",
    "            train_loss, train_acc, weight_arr_updated, bias_arr_updated = run_train(x=train_x,\n",
    "                                                                                            y=train_y,\n",
    "                                                                                            weight_arr=weight_arr,\n",
    "                                                                                            bias_arr=bias_arr)\n",
    "\n",
    "            # update weigth and bias\n",
    "            weight_arr = weight_arr_updated\n",
    "            bias_arr = bias_arr_updated\n",
    "\n",
    "            loss_list.append(train_loss)\n",
    "            acc_list.append(train_acc)\n",
    "\n",
    "        if (report > 0) & ((epoch + 1) % report == 0):\n",
    "            test_acc = run_test(x=test_x, y=test_y, weight_arr=weight_arr, bias_arr=bias_arr)\n",
    "            print(f'''\n",
    "                Epoch: {epoch+1}\n",
    "                Train loss mean: {np.mean(loss_list)}\n",
    "                Train accuracy mean: {np.mean(acc_list)}\n",
    "                Test accuracy: {test_acc}''')\n",
    "    final_acc = run_test(x=test_x, y=test_y,\n",
    "                           weight_arr=-weight_arr, bias_arr=bias_arr)\n",
    "    print(f\"\"\"\n",
    "    Final Test\n",
    "    Accuracy = {final_acc}\"\"\")\n",
    "\n",
    "\n",
    "def arange_data(data, mb_size):\n",
    "    \"\"\"shuffle data\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    mb_size: int\n",
    "        mini batch size\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    shuffle_map: numpy ndarray\n",
    "    step_count: int\n",
    "        number of batch steps for each epoch\n",
    "    test_begin_idx: int\n",
    "    \"\"\"\n",
    "\n",
    "    shuffle_map = np.arange(data.shape[0])  # get row index\n",
    "    np.random.shuffle(shuffle_map)  # shuffle row index\n",
    "    step_cnt = int(data.shape[0] * .8 // mb_size)  # get step count(train 80%)\n",
    "    test_begin_idx = step_cnt * mb_size\n",
    "\n",
    "    return shuffle_map, step_cnt, test_begin_idx\n",
    "\n",
    "\n",
    "def get_train_data(data, mb_size, shuffle_map, test_begin_idx, output_cnt, step_idx):\n",
    "    \"\"\"get train data\n",
    "    get mini batch data for the batch step\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    data: numpy ndarray\n",
    "    mb_size: int\n",
    "    shuffle_mape: numpy ndarray\n",
    "    test_begin_idx: int\n",
    "    output_cnt: int\n",
    "    step_idx: int\n",
    "    \"\"\"\n",
    "    if step_idx == 0:\n",
    "        # shuffle and remove test index for each epoch\n",
    "        np.random.shuffle(shuffle_map[:test_begin_idx])\n",
    "    # get batch train data\n",
    "    train_data = data[shuffle_map[mb_size * step_idx:mb_size * (step_idx + 1)]]\n",
    "    train_x, train_y = train_data[:, : -output_cnt], train_data[:, -output_cnt:]\n",
    "    return train_x, train_y\n",
    "\n",
    "\n",
    "def get_test_data(data, shuffle_map, test_begin_idx, output_cnt):\n",
    "    \"\"\"get test data\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    data: numpy ndarray\n",
    "    shuffle_map: numpy array\n",
    "        shuffled index of the data\n",
    "    test_begin_idx: int\n",
    "    output_cnt: int\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    test_x: numpy ndarray\n",
    "    test_y: numpy ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    test_data = data[shuffle_map[test_begin_idx:]]\n",
    "    test_x, test_y = test_data[:, :-output_cnt], test_data[:, -output_cnt:]\n",
    "    return test_x, test_y\n",
    "\n",
    "\n",
    "def run_train(x, y, weight_arr, bias_arr):\n",
    "    # foward\n",
    "    # get out_y and return out_y and x\n",
    "    out_y, x = foward_nn(x=x, weight_arr=weight_arr, bias_arr=bias_arr)\n",
    "    loss, diff = forward_postproc(out_y=out_y, y=y)  # get loss\n",
    "    accuracy = eval_accuracy(out_y=out_y, y=y)\n",
    "\n",
    "    # back\n",
    "    dl_dout = 1\n",
    "    dl_dout = backprop_postproc(dl_dout_arr=dl_dout, diff_arr=diff)\n",
    "    weight_arr_updated, bias_arr_updated = backprop_nn(dl_dout_arr=dl_dout, x=x,\n",
    "                                                            weight_arr=weight_arr, bias_arr=bias_arr)\n",
    "\n",
    "    return loss, accuracy, weight_arr_updated, bias_arr_updated\n",
    "\n",
    "\n",
    "def run_test(x, y, weight_arr, bias_arr):\n",
    "    output, x = foward_nn(x=x, weight_arr=weight_arr, bias_arr=bias_arr)\n",
    "    accuracy = eval_accuracy(output, y)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def foward_nn(x, weight_arr, bias_arr):\n",
    "    \"\"\"foward neuralnet\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    x: numpy ndarray\n",
    "        n_rows: number of batches\n",
    "        n_cols: number of features\n",
    "    weight_arr: numpy ndarray\n",
    "        n_rows: number of features\n",
    "        n_cols: number of layers\n",
    "    bias_arr: numpy adarray\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    out_y: numpy ndarry\n",
    "        n_rows: number of features\n",
    "        n_cols: number of batches\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    out_y = np.matmul(x, weight_arr) + bias_arr\n",
    "\n",
    "    return out_y, x\n",
    "\n",
    "\n",
    "def forward_postproc(out_y, y):\n",
    "    \"\"\"foward neuralnet post process\n",
    "    get differents, square, loss\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    out_y: numpy ndarray\n",
    "    y: numpy ndarray\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    loss: float\n",
    "    diff: float\n",
    "    \"\"\"\n",
    "    diff = out_y - y\n",
    "    square = np.square(diff)\n",
    "    loss = np.mean(square)\n",
    "\n",
    "    return loss, diff\n",
    "\n",
    "\n",
    "def backprop_nn(dl_dout_arr, weight_arr, bias_arr, x):\n",
    "    \"\"\"back propagatagtion\n",
    "    update weight, bias with gradiant L\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    dl_dout_arr: numpy ndarray\n",
    "        순전파 출력에 대한 Loss 기울기 (dl/dout)\n",
    "        r_row : number of batches\n",
    "        n_col: number of output\n",
    "\n",
    "    weight_arr: numpy ndarray\n",
    "        n_row: number of features\n",
    "        n_col: number of output\n",
    "\n",
    "    bias_arr: numpy ndarray\n",
    "        n_row: number of batches\n",
    "        n_col: number of output\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    weight_arr: numpy ndarray\n",
    "    bias_arr: numpy ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    dout_dw_arr = x.transpose()  # dout/dw\n",
    "    dl_dw_arr = np.matmul(dout_dw_arr, dl_dout_arr)  # dl/dout\n",
    "\n",
    "    dl_db_arr = np.sum(dl_dout_arr)  # dl/db\n",
    "\n",
    "    # update weight, bias with learning rate\n",
    "    weight_arr = LEARNING_RATE * dl_dw_arr\n",
    "    bias_arr = LEARNING_RATE * dl_db_arr\n",
    "\n",
    "    return weight_arr, bias_arr\n",
    "\n",
    "\n",
    "def backprop_postproc(dl_dout_arr, diff_arr):\n",
    "    \"\"\"back propagation post porcess\n",
    "    loss(mean) -> square -> diff -> output\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    diff: numpy ndarray\n",
    "    Returns\n",
    "    --------\n",
    "    \"\"\"\n",
    "\n",
    "    dl_dmean = np.ones(diff_arr.shape) / np.prod(diff_arr.shape)\n",
    "    dmean_ddiff = 2 * diff_arr\n",
    "    ddiff_dy = 1\n",
    "\n",
    "    dl_dout = dl_dout_arr * dl_dmean * dmean_ddiff * ddiff_dy\n",
    "\n",
    "    return dl_dout\n",
    "\n",
    "\n",
    "def eval_accuracy(y, out_y):\n",
    "    return 1 - np.mean(np.abs((out_y-y)/y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Epoch: 1\n",
      "                Train loss mean: 110.5374732377819\n",
      "                Train accuracy mean: -0.005895754192065463\n",
      "                Test accuracy: -174.92405227833706\n",
      "\n",
      "    Final Test\n",
      "    Accuracy = -2521.6941210518794\n"
     ]
    }
   ],
   "source": [
    "dataset = load_data_set(path=data_path, input_cnt=10, output_cnt=1)\n",
    "weight_arr, bias_arr = init_model(rnd_mean=RND_MEAN, rnd_std=RND_STD, input_cnt=10, output_cnt=1)\n",
    "train_and_test(data=dataset, epoch_cnt=1, mb_size=5, report=1,\n",
    "                 weight_arr=weight_arr, bias_arr=bias_arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
