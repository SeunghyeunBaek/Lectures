# Chapter 1

> 회귀분석: 전복의 고리 수 추정 신경망
>
> 출처: 파이썬 날코딩으로 알고 짜는 딥러닝

## Keywords

- 파라미터
  - 학습 과정 중에 끊임없이 변경되어 가면서 퍼셉트론의 동작 특성을 결정하는 값들로 모델 파라미터라고 부르기도 한다.
- 텐서
  - 다차원 숫자 배열
- 미니배치
  - 배치 작업보다 작은 단위로 처리하는 일괄처리. 데이터 처리의 효율을 높여주며 개별 학습 데이터의 특징을 무시하지 않으면서도 특징에 너무 휘둘리지 않게 해준다.
- 선형 연산, 비선형 연산
  - 입력 성분의 일차식으로 표현되는 계산 과정을 선형 연산
  - 일차식으로 나타낼 수 없는 계산 과정을 비선형 연산
- 에포크
  - 학습 데이터 전체에 대한 한 차례 처리
- 하이퍼파라미터
  - 에폭 수나 미니배치 크기처럼 학습 과정에서 변경되지 않으면서 신경망 구조나 학습 결과에 영향을 미치는 고려 요인들
- 손실 함수, 비용 함수
  - 값이 항상 0 이상이며 추정이 정확해질수록 값이 작아지는 성질이 있으면서 미분도 가능한 성질의 평가지표
- 학습과 훈련
  - 같은 행위를 정반대의 입장에서 보고 표현한 것. 데이터를 처리해 신경망을 개선시키는 과정을 신경망 입장에서는 '데이터를 학습', 개발자 입장에서는 '신경망을 훈련'
- 순전파
  - 입력 데이터에 대해 신경망 구조를 따라가면서 현재의 파라미터값들을 이용해 손실 함숫값을 계산하는 과정
- 역전파
  - 순전파의 계산 과정을 역순으로 거슬러가면서 손실 함숫값에 직간접적으로 영향을 미친 모든 성분에 대하여 손실 기울기를 계산하는 과정

## Theory

1. 단층 퍼셉트론 신경망의 구조
2. 텐서 연산, 미니배치, 하이퍼파라미터
3. 신경망의 세 가지 기본 출력 유형
4. 회귀 분석, 평균제곱오차(MSE) 손실 함수
5. 경사하강법, 역전파, 편미분, 손실 기울기

### 1. 단층 퍼셉트론 신경망의 구조

단층 퍼셉트론은 다음 그림처럼 일련의 퍼셉트론을 한 줄로 배치하여 입력 벡터 하나로부터 출력 벡터 하나를 얻는 가장 기본적인 신경망 구조이다. 아래 그림의 $P_1, P_2, P_3$가 퍼셉트론을 의미한다.

<img src="assets/image-20200423181418605.png" alt="image-20200423181418605" style="zoom:50%;" />

그림에서 보는 것과 같이 퍼셉트론 사이에는 어떤 연결도 없어 서로 영향을 주고받을 수 없다. 퍼셉트론은 저마다의 가중치 벡터($w_i$)와 편향값($b_i$)을 이용하여 입력 벡터로부터 출력 벡터를 도출한다. 

이처럼 단층 퍼셉트론에서 퍼셉트론들은 입력 벡터만 공유할 뿐 각자의 가중치 벡터와 편향값에 따라 각자의 방식으로 독립적인 정보를 따로따로 생산한다. 이 때, 이들 퍼셉트론의 가중치 벡터들을 한데 모은 것이 가중치 행렬 $W$이고 편향값들을 한데 모은 것이 편향 벡터 $b$이다.

딥러닝의 학습 목표는 문제 풀이에 적합한 파라미터값의 조합을 구하는 것인데, 단층 퍼셉트론에서는 적합한 $W, b$ 값을 찾는 것이라고 할 수 있다.

### 2. 텐서 연산, 미니배치, 하이퍼파라미터

딥러닝 프로그래밍에서 텐서의 이해와 활용은 아주 중요하다. 텐서를 엄밀하게 정의하기란 어렵지만 딥러닝에서는 다차원 숫자 배열 정도로 이해해도 큰 문제가 없다. 0차원 스칼라, 1차원 벡터, 2차원 행렬이 모두 텐서이며, 3차원 이상의 숫자 배열 역시 텐서다.

딥러닝에서 텐서가 중요한 이유는 같은 문제라도 반복문 대신 텐서를 이용해 처리하는 편이 **프로그램도 간단하고 처리 속도도 훨씬 빠르기 때문**이다. 이는 파이썬 인터프리터가 반복문보다 텐서 연산을 더 효율적으로  처리할 수 있기 때문이며, 특히 병렬 수치 연산을 지원하는 GPU 이용 환경에서 속도 차이는 더욱 커진다.

일반적으로 딥러닝에서 신경망이 여러 데이터를 한꺼번에 처리하는데 이를 미니배치라고 한다.

<img src="assets/image-20200423183551162.png" alt="image-20200423183551162" style="zoom:50%;" />

퍼셉트론은 입력 벡터로부터 스칼라 출력 $y=x_1w_1+\dots+x_nw_n=xw+b$를 계산한다. 이 때 반복문을 이용해 $x_iw_i$들을 모두 계산하고 합산해서 $y$를 구할 수도 있지만 그보다는 벡터의 내적 연산을 이용해 단번에 계산하는 편이 훨씬 간단하고 빠르다. 다시 말해서 행렬곱을 사용하면 훨씬 빠르게 계산할 수 있다. 한편 입력 성분의 일차식으로 표현되는 이런 계산 과정을 선형 연산이라하며, 일차식으로 나타낼 수 없는 계산 과정을 비선형 연산이라 한다.

그림 c는 미니배치 처리에서의 단층 퍼셉트론의 동작 방식을 나타내는데, 데이터가 여러 개 모여서 입력 행렬 $X$와 출력 행렬 $Y$가 될 뿐 일련의 퍼셉트론들이 각 데이터에 공통적으로 적용되어 $y_i=x_iW+b$를 계산하는 것이다. 이 미니배치 처리는 $Y=XW+b$의 식으로 요약이 가능하며 역시 반복문으로 그림 a,b의 동작을 반복하기보다 행렬 연산을 사용하여 단번에 계산하는 편이 효율적이다.

이처럼 **딥러닝 프로그램의 효율을 높이려면 최대한 반복문 사용을 피하고 텐서 연산을 이용해 처리하는 것이 중요**하다.

에포크 수나 미니배치 크기처럼 학습 과정에서 변경되지 않으면서 신경망 구조나 학습 결과에 영향을 미치는 고려 요인들을 하이퍼파라미터라고 한다.  이 값은 신경망 설계자가 학습 전에 미리 정해주어야 하는 값이며 학습 결과에 큰 영향을 미치는 경우가 많다 .따라서 설계자는 문제 유형, 신경망 구조, 데이터양, 학습 결과 등을 종합적으로 살펴보며 이 값들을 잘 조절해야 한다.

### 3. 신경망의 세 가지 기본 출력 유형

인공지능 알고리즘의 출력 내용을 세분하면 회귀 분석, 이진 판단, 선택 분류로 구성된다. 이진 판단과 선택 분류는 추후에 다르고 여기서는 회귀 분석에 관한 내용만 다루도록 한다.

통계학에서는 연속형 변수 사이의 모형을 구한 뒤 적합도를 측정하는 분석 방법을 회귀 분석이라고 한다. 입력으로 주어진 값들을 근거로 미지의 변숫값을 추정하고 예측하는 데 주로 이용된다. 딥러닝 알고리즘의 값 추정 역시 신경망 모델이 입력 데이터를 근거로 출력값을 추정하는 것이므로 회귀 분석의 한 방법에 해당한다.

### 4. 회귀 분석, 평균제곱오차(MSE) 손실 함수

회귀 분석 출력을 내는 딥러닝 신경망은 값의 추정에 근거가 되는 입력 데이터가 잘 주어질수록 그리고 신경망 구조가 이 데이터에 적합한 구조이고 학습이 잘 될수록 더 정확한 추정값을 만든다.

그런데 딥러닝 학습을 하려면 추정값이 따로 주어지는 정답에 비교할 때 얼마나 정확한지를 숫자 하나로 요약해서 보여주는 정량적 지표가 필요하다. 회귀 분석에서는 추정값이 얼마나 정확한지 평가할 때 보통 평균제곱오차(MSE)를 평가 지표로 삼는다. 평균제곱오차란 출력 각 성분에 대해 추정값과 정답 사이의 차이인 오차를 제곱한 뒤 모두 합해 전체 성분 수로 나눈 값이다.
$$
\sum^n_i(y_i-\hat y_i)^2 / n
$$
딥러닝에서는 **값이 항상 0 이상이며 추정이 정확해질수록 값이 작아지는 성질이 있으면서 미분도 가능한 평가지표를 정의한 후 이를 최소화하는 것을 목표로 학습을 수행**한다.  이런 성질의 평가지표를 손실 함수(loss function) 혹은 비용함수(cost function)라고 부른다. 회귀 분석 출력에 대한 평균제곱오차는 계산하기 쉬우면서도 딥러닝이 요구하는 성질을 충족시켜 손실 함수로 유용하게 이용된다.

### 5. 경사하강법, 역전파, 편미분, 손실 기울기

#### 경사하강법

경사하강법은 함수의 기울기를 반복 계산하면서 이 기울기에 따라 함숫값이 낮아지는 방향으로 이동하는 기본적인 딥러닝 학습 알고리즘이다.

경사하강법은 미니배치 입력 데이터에 대해 순전파와 역전파 과정을 번갈아 수행하는 과정을 반복하면서 신경망 파라미터들을 원하는 방향으로 바꾸어나간다. 역전파에서는 딥러닝 알고리즘이 값을 바꿀 수 있는 대상인 파라미터 성분에 대해서 계산된 손실 기울기를 이용해 실제로 그 값을 변경하는데, 이 변경으로 인해 신경망의 변화, 즉 학습이 일어난다. 경사하강법의 경우 손실 기울기에 미리 정해진 학습률(learning rate)이라는 하이퍼파라미터값을 곱한 값을 빼는 가장 간단한 방법으로 파라미터값을 변경한다.

<img src="assets/image-20200423192511796.png" alt="image-20200423192511796" style="zoom:33%;" />

그래프 전체를 볼 수 없는 상황에서 $f(x)$값이 최소가 되는 $x$값을 찾아야 한다고 가정하자. 그래프의 경사는 $f'(x)$에 해당하므로 이에 비례하는 값을 $x$에서 빼주는 처리를 하면 이 모든 처리가 가능해진다.

따라서 위 그림에서 단 한 걸음 속의 관계를 식으로 나타내면
$$
x_{i+1} = x_i - \alpha\frac{\partial f(x)}{\partial x}
$$
이 된다. 이 때 비례상수 $\alpha$를 학습률이라고 하며 임의의 양수값을 사용할 수 있지만 값이 클수록 목표 근처에서 정확하게 바닥을 찾는 능력이 무디어지고 값이 작을수록 바닥점 근처에 접근하는 시간이 더 오래 걸린다. 여기서 전미분 $df(x)$ 대신 편미분 $\partial f(x)$를 쓰는 이유는 다음과 같다.

1. 미분은 대부분 계산 자체가 불가능하거나 몹시 어렵지만, 편미분은 상대적으로 쉽게 계산할 수 있다.
2. 편미분 과정에서 상수처럼 취급되는 다른 변수도 같은 편미분 방식으로 처리해서 이들을 잘 종합하면 전체적으로 전미분과 같은 효과를 낼 수 있다.

딥러닝 모델에서 손실 함숫값은 매우 많은 성분에 의해 계산되기 때문에 이를 그래프로 나타내면 고차원의 그래프로서 그림으로 나타낼 수 없는 지경이 된다(정확히 말하자면 모델이 갖는 전체 파라미터 개수만큼의 차원을 갖는 엄청난 고차원 공간). 하지만 경사하강법에서는 2차원 등산로에서와 마찬가지 방법으로 고차원 등산로를 한 발 한 발 탐색해 손실 함숫값이 최소화되는 바닥점을 찾아간다.

안타깝게도 경사하강법으로 항상 최적의 바닥점에 도달할 수 있다는 보장은 없다. 단지 현재 위치를 둘러싼 주위의 지형에 비해 작은 함숫값을 갖는 이른바 지역적 바닥점(local minimum)에 도달할 수 있을 뿐이며 이 때문에 경사하강법에 대한 여러가지 개선책과 보조 기법들이 제안되고 있다.

#### 편미분과 손실 기울기의 계산

역전파 과정에서 손실 함숫값에 직간접적으로 영향을 미친 모든 성분에 대하여 손실 기울기를 계산한다고 했는데 이는 편미분의 연쇄적 관계 덕분이다. 편미분의 연쇄적 관계는 미분의 기본 성질인 $\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y}\frac{\partial y}{\partial x}$의 수식을 의미한다.

<img src="assets/image-20200423194728624.png" alt="image-20200423194728624" style="zoom:50%;" />

우리가 관심을 갖는 5는 2의 역전파 과정으로서 $y=f(x)$의 성질과 순전파에서 실제로 구해진 $x,y$값을 이용하여 $\frac{\partial y}{\partial x} = \frac{\partial f(x)}{\partial x}$값을 계산한 후 여기에 전달받은 $\frac{\partial L}{\partial y}$값을 곱해 $\frac{\partial L}{\partial x}$값을 구한다.

역전파 과정 중에 가중치나 편향 같은 파라미터 성분에 대해서는 해당 성분의 손실 기울기에 학습률을 곱한 값을 빼줌으로써 그 값을 변경시킨다. 바로 이 부분이 학습이 실제로 일어나는 지점이다. 이때 학습률은 학습 속도를 조절하는 매우 중요한 하이퍼파라미터값으로서 너무 크거나 작지 않게 적절한 값을 설정해줄 필요가 있다.

<img src="assets/image-20200423195501302.png" alt="image-20200423195501302" style="zoom:50%;" />

위의 그림과 같이 지나치게 작은 학습률은 손실 함숫값이 최소가 되는 바닥점에 도달하기까지 너무 긴학습이 필요하게 된다. 반면 지나치게 큰 학습률은 손실 함숫값이 바닥점에 도달하지 못한 채 바닥점 근처를 맴돌게 만들 가능성이 크다.



